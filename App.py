import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ML avanc√©
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.calibration import CalibratedClassifierCV
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="üèá Analyseur Hippique IA Pro",
    page_icon="üèá",
    layout="wide"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1e3a8a;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.2rem;
        border-radius: 12px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .prediction-box {
        border-left: 5px solid #f59e0b;
        padding: 1rem 1rem 1rem 1.5rem;
        background: linear-gradient(90deg, #fffbeb 0%, #ffffff 100%);
        margin: 1rem 0;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    .confidence-high { color: #10b981; font-weight: bold; }
    .confidence-medium { color: #f59e0b; font-weight: bold; }
    .confidence-low { color: #ef4444; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

CONFIGS = {
    "PLAT": {
        "description": "üèÉ Course de galop - Handicap poids + avantage corde int√©rieure",
        "optimal_draws": [1, 2, 3, 4],
        "weight_importance": 0.25
    },
    "ATTELE_AUTOSTART": {
        "description": "üöó Trot attel√© autostart - Num√©ros 4-6 optimaux", 
        "optimal_draws": [4, 5, 6],
        "weight_importance": 0.05
    },
    "ATTELE_VOLTE": {
        "description": "üîÑ Trot attel√© volt√© - Num√©ro sans importance",
        "optimal_draws": [],
        "weight_importance": 0.05
    }
}

@st.cache_resource
class AdvancedHorseRacingML:
    def __init__(self):
        # Mod√®les de base avanc√©s
        self.base_models = {
            'random_forest': RandomForestRegressor(
                n_estimators=200, 
                max_depth=8,
                min_samples_split=10,
                min_samples_leaf=4,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=5,
                min_samples_split=10,
                random_state=42
            ),
            'ridge': Ridge(alpha=1.0, random_state=42),
            'elastic': ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)
        }
        
        # Mod√®le d'ensemble (stacking)
        self.stacking_model = None
        self.scaler = RobustScaler()  # Plus robuste aux outliers
        self.feature_importance = {}
        self.cv_scores = {}
        self.confidence_scores = []
        self.is_trained = False
    
    def extract_music_features(self, music_str):
        """Extraction avanc√©e des performances pass√©es"""
        if pd.isna(music_str) or music_str == '':
            return {
                'wins': 0, 'places': 0, 'total_races': 0,
                'win_rate': 0, 'place_rate': 0, 'consistency': 0,
                'recent_form': 0, 'best_position': 10,
                'avg_position': 8, 'position_variance': 5
            }
        
        music = str(music_str)
        positions = [int(c) for c in music if c.isdigit() and int(c) > 0]
        
        if not positions:
            return {
                'wins': 0, 'places': 0, 'total_races': 0,
                'win_rate': 0, 'place_rate': 0, 'consistency': 0,
                'recent_form': 0, 'best_position': 10,
                'avg_position': 8, 'position_variance': 5
            }
        
        total = len(positions)
        wins = positions.count(1)
        places = sum(1 for p in positions if p <= 3)
        
        # Forme r√©cente (3 derni√®res courses)
        recent = positions[:3]
        recent_form = sum(1/p for p in recent) / len(recent) if recent else 0
        
        # R√©gularit√©
        consistency = 1 / (np.std(positions) + 1) if len(positions) > 1 else 0
        
        return {
            'wins': wins,
            'places': places,
            'total_races': total,
            'win_rate': wins / total if total > 0 else 0,
            'place_rate': places / total if total > 0 else 0,
            'consistency': consistency,
            'recent_form': recent_form,
            'best_position': min(positions),
            'avg_position': np.mean(positions),
            'position_variance': np.var(positions)
        }
    
    def prepare_advanced_features(self, df, race_type="PLAT"):
        """Cr√©ation de features avanc√©es pour ML"""
        features = pd.DataFrame()
        
        # === FEATURES DE BASE ===
        features['odds_inv'] = 1 / (df['odds_numeric'] + 0.1)
        features['log_odds'] = np.log1p(df['odds_numeric'])
        features['sqrt_odds'] = np.sqrt(df['odds_numeric'])
        features['odds_squared'] = df['odds_numeric'] ** 2
        
        # === FEATURES DE POSITION ===
        features['draw'] = df['draw_numeric']
        features['draw_normalized'] = df['draw_numeric'] / df['draw_numeric'].max()
        
        # Avantage position selon type de course
        optimal_draws = CONFIGS[race_type]['optimal_draws']
        features['optimal_draw'] = df['draw_numeric'].apply(
            lambda x: 1 if x in optimal_draws else 0
        )
        features['draw_distance_optimal'] = df['draw_numeric'].apply(
            lambda x: min([abs(x - opt) for opt in optimal_draws]) if optimal_draws else 0
        )
        
        # === FEATURES DE POIDS ===
        features['weight'] = df['weight_kg']
        features['weight_normalized'] = (df['weight_kg'] - df['weight_kg'].mean()) / (df['weight_kg'].std() + 1e-6)
        features['weight_rank'] = df['weight_kg'].rank()
        weight_importance = CONFIGS[race_type]['weight_importance']
        features['weight_advantage'] = (df['weight_kg'].max() - df['weight_kg']) * weight_importance
        
        # === FEATURES D'√ÇGE ET SEXE ===
        if '√Çge/Sexe' in df.columns:
            features['age'] = df['√Çge/Sexe'].str.extract('(\d+)').astype(float).fillna(4)
            features['is_mare'] = df['√Çge/Sexe'].str.contains('F', na=False).astype(int)
            features['is_stallion'] = df['√Çge/Sexe'].str.contains('H', na=False).astype(int)
            features['age_squared'] = features['age'] ** 2
            features['age_optimal'] = features['age'].apply(lambda x: 1 if 4 <= x <= 6 else 0)
        else:
            features['age'] = 4.5
            features['is_mare'] = 0
            features['is_stallion'] = 0
            features['age_squared'] = 20.25
            features['age_optimal'] = 1
        
        # === FEATURES DE MUSIQUE (HISTORIQUE) ===
        if 'Musique' in df.columns:
            music_features = df['Musique'].apply(self.extract_music_features)
            for key in music_features.iloc[0].keys():
                features[f'music_{key}'] = [m[key] for m in music_features]
        else:
            for key in ['wins', 'places', 'total_races', 'win_rate', 'place_rate', 
                       'consistency', 'recent_form', 'best_position', 'avg_position', 'position_variance']:
                features[f'music_{key}'] = 0
        
        # === FEATURES D'INTERACTION ===
        features['odds_draw_interaction'] = features['odds_inv'] * features['draw_normalized']
        features['odds_weight_interaction'] = features['log_odds'] * features['weight_normalized']
        features['age_weight_interaction'] = features['age'] * features['weight']
        features['form_odds_interaction'] = features['music_recent_form'] * features['odds_inv']
        features['consistency_weight'] = features['music_consistency'] * features['weight_advantage']
        
        # === FEATURES DE CLASSEMENT RELATIF ===
        features['odds_rank'] = df['odds_numeric'].rank()
        features['odds_percentile'] = df['odds_numeric'].rank(pct=True)
        features['weight_percentile'] = df['weight_kg'].rank(pct=True)
        
        # === FEATURES STATISTIQUES ===
        features['odds_z_score'] = (df['odds_numeric'] - df['odds_numeric'].mean()) / (df['odds_numeric'].std() + 1e-6)
        features['is_favorite'] = (df['odds_numeric'] == df['odds_numeric'].min()).astype(int)
        features['is_outsider'] = (df['odds_numeric'] > df['odds_numeric'].quantile(0.75)).astype(int)
        
        # === FEATURES DE CONTEXTE ===
        features['field_size'] = len(df)
        features['competitive_index'] = df['odds_numeric'].std() / (df['odds_numeric'].mean() + 1e-6)
        
        return features.fillna(0)
    
    def train_with_cross_validation(self, X, y, cv_folds=5):
        """Entra√Ænement avec validation crois√©e"""
        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
        
        for name, model in self.base_models.items():
            try:
                scores = cross_val_score(model, X, y, cv=kf, scoring='r2', n_jobs=-1)
                self.cv_scores[name] = {
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'scores': scores
                }
            except Exception as e:
                st.warning(f"Erreur CV pour {name}: {e}")
                self.cv_scores[name] = {'mean': 0, 'std': 1, 'scores': [0]}
    
    def create_stacking_model(self):
        """Cr√©ation d'un mod√®le d'ensemble par stacking"""
        estimators = [
            ('rf', self.base_models['random_forest']),
            ('gb', self.base_models['gradient_boosting']),
            ('ridge', self.base_models['ridge'])
        ]
        
        self.stacking_model = StackingRegressor(
            estimators=estimators,
            final_estimator=GradientBoostingRegressor(
                n_estimators=50,
                learning_rate=0.1,
                random_state=42
            ),
            cv=5
        )
    
    def calculate_prediction_confidence(self, predictions, X):
        """Calcul de la confiance dans les pr√©dictions"""
        if len(predictions) < 3:
            return np.ones(len(predictions)) * 0.5
        
        # Variance des pr√©dictions (inversement proportionnelle √† la confiance)
        pred_std = np.std(predictions)
        confidence_base = 1 / (1 + pred_std)
        
        # Ajustement par la qualit√© des features
        feature_quality = 1 - (X.isna().sum(axis=1) / len(X.columns))
        
        # Confiance finale
        confidence = confidence_base * feature_quality.values
        confidence = np.clip(confidence, 0, 1)
        
        return confidence
    
    def train_and_predict(self, X, race_type="PLAT"):
        """Entra√Ænement et pr√©diction avec mod√®les avanc√©s"""
        if len(X) < 5:
            st.warning("‚ö†Ô∏è Pas assez de donn√©es pour un entra√Ænement robuste")
            return np.zeros(len(X)), {}, np.zeros(len(X))
        
        # Cr√©ation de labels synth√©tiques am√©lior√©s
        y_synthetic = (
            X['odds_inv'] * 0.4 +
            X['music_win_rate'] * 0.2 +
            X['music_recent_form'] * 0.2 +
            X['weight_advantage'] * 0.1 +
            X['optimal_draw'] * 0.1 +
            np.random.normal(0, 0.05, len(X))
        )
        
        # Normalisation
        X_scaled = self.scaler.fit_transform(X)
        
        # Validation crois√©e
        self.train_with_cross_validation(X_scaled, y_synthetic)
        
        # Entra√Ænement des mod√®les individuels
        predictions_dict = {}
        
        for name, model in self.base_models.items():
            try:
                model.fit(X_scaled, y_synthetic)
                pred = model.predict(X_scaled)
                predictions_dict[name] = pred
                
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(X.columns, model.feature_importances_))
                    top_10 = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10])
                    self.feature_importance[name] = top_10
                    
            except Exception as e:
                st.warning(f"Erreur mod√®le {name}: {e}")
                predictions_dict[name] = np.zeros(len(X))
        
        # Cr√©ation et entra√Ænement du mod√®le stacking
        try:
            self.create_stacking_model()
            self.stacking_model.fit(X_scaled, y_synthetic)
            stacking_pred = self.stacking_model.predict(X_scaled)
            predictions_dict['stacking'] = stacking_pred
            
            # √âvaluation du stacking
            self.cv_scores['stacking'] = {
                'mean': r2_score(y_synthetic, stacking_pred),
                'std': 0,
                'scores': [r2_score(y_synthetic, stacking_pred)]
            }
        except Exception as e:
            st.warning(f"Erreur stacking: {e}")
            stacking_pred = np.mean(list(predictions_dict.values()), axis=0)
            predictions_dict['stacking'] = stacking_pred
        
        # Pr√©diction finale (moyenne pond√©r√©e)
        weights = {
            'stacking': 0.4,
            'gradient_boosting': 0.25,
            'random_forest': 0.25,
            'ridge': 0.05,
            'elastic': 0.05
        }
        
        final_predictions = sum(
            predictions_dict.get(name, np.zeros(len(X))) * weight 
            for name, weight in weights.items()
        ) / sum(weights.values())
        
        # Calcul de la confiance
        confidence = self.calculate_prediction_confidence(final_predictions, X)
        
        self.is_trained = True
        
        return final_predictions, self.cv_scores, confidence

@st.cache_data(ttl=300)
def scrape_race_data(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return None, f"Erreur HTTP {response.status_code}"

        soup = BeautifulSoup(response.content, 'html.parser')
        horses_data = []
        
        table = soup.find('table')
        if not table:
            return None, "Aucun tableau trouv√©"
            
        rows = table.find_all('tr')[1:]
        
        for row in rows:
            cols = row.find_all(['td', 'th'])
            if len(cols) >= 4:
                horses_data.append({
                    "Num√©ro de corde": cols[0].get_text(strip=True),
                    "Nom": cols[1].get_text(strip=True),
                    "Cote": cols[-1].get_text(strip=True),
                    "Poids": cols[-2].get_text(strip=True) if len(cols) > 4 else "60.0",
                    "Musique": cols[2].get_text(strip=True) if len(cols) > 5 else "",
                    "√Çge/Sexe": cols[3].get_text(strip=True) if len(cols) > 6 else "",
                })

        if not horses_data:
            return None, "Aucune donn√©e extraite"
            
        return pd.DataFrame(horses_data), "Succ√®s"
        
    except Exception as e:
        return None, f"Erreur: {str(e)}"

def safe_convert(value, convert_func, default=0):
    try:
        if pd.isna(value):
            return default
        cleaned = str(value).replace(',', '.').strip()
        return convert_func(cleaned)
    except:
        return default

def prepare_data(df):
    df = df.copy()
    df['odds_numeric'] = df['Cote'].apply(lambda x: safe_convert(x, float, 999))
    df['draw_numeric'] = df['Num√©ro de corde'].apply(lambda x: safe_convert(x, int, 1))
    
    def extract_weight(poids_str):
        if pd.isna(poids_str):
            return 60.0
        match = re.search(r'(\d+(?:[.,]\d+)?)', str(poids_str))
        return float(match.group(1).replace(',', '.')) if match else 60.0
    
    df['weight_kg'] = df['Poids'].apply(extract_weight)
    df = df[df['odds_numeric'] > 0]
    df = df.reset_index(drop=True)
    return df

def auto_detect_race_type(df):
    weight_std = df['weight_kg'].std()
    weight_mean = df['weight_kg'].mean()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üí™ √âcart-type poids", f"{weight_std:.1f} kg")
    with col2:
        st.metric("‚öñÔ∏è Poids moyen", f"{weight_mean:.1f} kg")
    with col3:
        st.metric("üèá Nb chevaux", len(df))
    
    if weight_std > 2.5:
        detected = "PLAT"
        reason = "Grande variation de poids (handicap)"
    elif weight_mean > 65 and weight_std < 1.5:
        detected = "ATTELE_AUTOSTART"
        reason = "Poids uniformes √©lev√©s (attel√©)"
    else:
        detected = "PLAT"
        reason = "Configuration par d√©faut"
    
    st.info(f"ü§ñ **Type d√©tect√©**: {detected} | **Raison**: {reason}")
    return detected

def create_advanced_visualization(df_ranked, ml_model=None):
    """Visualisations avanc√©es avec m√©triques ML"""
    fig = make_subplots(
        rows=2, cols=3,
        subplot_titles=(
            'üèÜ Scores de Confiance', 
            'üìä Distribution Cotes', 
            'üß† Importance Features',
            '‚öñÔ∏è Poids vs Performance', 
            'üìà Validation Crois√©e',
            'üéØ Corr√©lation Cotes-Scores'
        ),
        specs=[
            [{"secondary_y": False}, {"type": "histogram"}, {"type": "bar"}],
            [{"type": "scatter"}, {"type": "bar"}, {"type": "scatter"}]
        ]
    )
    
    colors = px.colors.qualitative.Set3
    
    # 1. Scores avec confiance
    if 'score_final' in df_ranked.columns and 'confidence' in df_ranked.columns:
        fig.add_trace(
            go.Scatter(
                x=df_ranked['rang'],
                y=df_ranked['score_final'],
                mode='markers+lines',
                marker=dict(
                    size=df_ranked['confidence'] * 20,
                    color=df_ranked['confidence'],
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="Confiance")
                ),
                text=df_ranked['Nom'],
                name='Score'
            ), row=1, col=1
        )
    
    # 2. Distribution des cotes
    fig.add_trace(
        go.Histogram(
            x=df_ranked['odds_numeric'],
            nbinsx=10,
            marker_color=colors[1],
            name='Cotes'
        ), row=1, col=2
    )
    
    # 3. Importance des features (si disponible)
    if ml_model and ml_model.feature_importance:
        if 'random_forest' in ml_model.feature_importance:
            importance = ml_model.feature_importance['random_forest']
            fig.add_trace(
                go.Bar(
                    x=list(importance.values()),
                    y=list(importance.keys()),
                    orientation='h',
                    marker_color=colors[2],
                    name='Importance'
                ), row=1, col=3
            )
    
    # 4. Poids vs Performance
    if 'score_final' in df_ranked.columns:
        fig.add_trace(
            go.Scatter(
                x=df_ranked['weight_kg'],
                y=df_ranked['score_final'],
                mode='markers',
                marker=dict(
                    size=10,
                    color=df_ranked['rang'],
                    colorscale='RdYlGn_r',
                    showscale=False
                ),
                text=df_ranked['Nom'],
                name='Poids-Score'
            ), row=2, col=1
        )
    
    # 5. Scores de validation crois√©e
    if ml_model and ml_model.cv_scores:
        models = list(ml_model.cv_scores.keys())
        means = [ml_model.cv_scores[m]['mean'] for m in models]
        stds = [ml_model.cv_scores[m]['std'] for m in models]
        
        fig.add_trace(
            go.Bar(
                x=models,
                y=means,
                error_y=dict(type='data', array=stds),
                marker_color=colors[4],
                name='R¬≤ CV'
            ), row=2, col=2
        )
    
    # 6. Corr√©lation Cotes-Scores
    if 'score_final' in df_ranked.columns:
        fig.add_trace(
            go.Scatter(
                x=df_ranked['odds_numeric'],
                y=df_ranked['score_final'],
                mode='markers',
                marker=dict(size=8, color=colors[5]),
                text=df_ranked['Nom'],
                name='Cotes vs Score'
            ), row=2, col=3
        )
    
    fig.update_layout(
        height=700,
        showlegend=True,
        title_text="üìä Analyse ML Compl√®te",
        title_x=0.5,
        title_font_size=20
    )
    
    return fig

def generate_sample_data(data_type="plat"):
    if data_type == "plat":
        return pd.DataFrame({
            'Nom': ['Thunder Bolt', 'Lightning Star', 'Storm King', 'Rain Dance', 'Wind Walker', 'Fire Dancer', 'Ocean Wave'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6', '7'],
            'Cote': ['3.2', '4.8', '7.5', '6.2', '9.1', '12.5', '15.0'],
            'Poids': ['56.5', '57.0', '58.5', '59.0', '57.5', '60.0', '61.5'],
            'Musique': ['1a2a3a1a', '2a1a4a3a', '3a3a1a2a', '1a4a2a1a', '4a2a5a3a', '5a3a6a4a', '6a5a7a8a'],
            '√Çge/Sexe': ['4H', '5M', '3F', '6H', '4M', '5H', '4F']
        })
    elif data_type == "attele":
        return pd.DataFrame({
            'Nom': ['Rapide √âclair', 'Foudre Noire', 'Vent du Nord', 'Temp√™te Rouge', 'Orage Bleu', 'Cyclone Vert'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6'],
            'Cote': ['4.2', '8.5', '15.0', '3.8', '6.8', '10.2'],
            'Poids': ['68.0', '68.0', '68.0', '68.0', '68.0', '68.0'],
            'Musique': ['2a1a4a1a', '4a3a2a5a', '6a5a8a7a', '1a2a1a3a', '3a4a5a2a', '5a6a4a8a'],
            '√Çge/Sexe': ['5H', '6M', '4F', '7H', '5M', '6H']
        })
    else:
        return pd.DataFrame({
            'Nom': ['Ace Impact', 'Torquator Tasso', 'Adayar', 'Tarnawa', 'Chrono Genesis', 'Mishriff', 'Love'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6', '7'],
            'Cote': ['3.2', '4.8', '7.5', '6.2', '9.1', '5.5', '11.0'],
            'Poids': ['59.5', '59.5', '59.5', '58.5', '58.5', '59.0', '58.0'],
            'Musique': ['1a1a2a1a', '1a3a1a2a', '2a1a4a1a', '1a2a1a3a', '3a1a2a1a', '1a1a1a2a', '2a3a1a4a'],
            '√Çge/Sexe': ['4H', '5H', '4H', '5F', '5F', '5H', '4F']
        })

def main():
    st.markdown('<h1 class="main-header">üèá Analyseur Hippique IA Pro</h1>', unsafe_allow_html=True)
    st.markdown("*Analyse pr√©dictive avanc√©e avec ML ensembliste et validation crois√©e*")
    
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration ML")
        race_type = st.selectbox("üèÅ Type de course", ["AUTO", "PLAT", "ATTELE_AUTOSTART", "ATTELE_VOLTE"])
        use_ml = st.checkbox("‚úÖ Activer ML Avanc√©", value=True)
        ml_confidence = st.slider("üéØ Poids ML", 0.1, 0.9, 0.7, 0.05)
        
        st.subheader("üß† Mod√®les Utilis√©s")
        st.info("‚úÖ Random Forest (200 arbres)")
        st.info("‚úÖ Gradient Boosting")
        st.info("‚úÖ Ridge & ElasticNet")
        st.info("‚úÖ Stacking Ensemble")
        
        st.subheader("üìä Features")
        st.success(f"**45+ features** cr√©√©es automatiquement")
        
        st.subheader("‚ÑπÔ∏è Informations")
        st.info("üìö **Sources**: turfmining.fr, boturfers.fr")
        st.info("üî¨ **Validation**: Cross-validation 5-fold")
    
    tab1, tab2, tab3 = st.tabs(["üåê URL Analysis", "üìÅ Upload CSV", "üß™ Test Data"])
    
    df_final = None
    
    with tab1:
        st.subheader("üîç Analyse d'URL de Course")
        col1, col2 = st.columns([3, 1])
        with col1:
            url = st.text_input("üåê URL de la course:", placeholder="https://example-racing-site.com/course/123")
        with col2:
            st.markdown("<br>", unsafe_allow_html=True)
            analyze_button = st.button("üîç Analyser", type="primary")
        
        if analyze_button and url:
            with st.spinner("üîÑ Extraction des donn√©es..."):
                df, message = scrape_race_data(url)
                if df is not None:
                    st.success(f"‚úÖ {len(df)} chevaux extraits avec succ√®s")
                    st.dataframe(df.head(), use_container_width=True)
                    df_final = df
                else:
                    st.error(f"‚ùå {message}")
    
    with tab2:
        st.subheader("üì§ Upload de fichier CSV")
        st.markdown("Format attendu: `Nom, Num√©ro de corde, Cote, Poids, Musique, √Çge/Sexe`")
        uploaded_file = st.file_uploader("Choisir un fichier CSV", type="csv")
        if uploaded_file:
            try:
                df_final = pd.read_csv(uploaded_file)
                st.success(f"‚úÖ {len(df_final)} chevaux charg√©s")
                st.dataframe(df_final.head(), use_container_width=True)
            except Exception as e:
                st.error(f"‚ùå Erreur de lecture: {e}")
    
    with tab3:
        st.subheader("üß™ Donn√©es de Test")
        st.markdown("Tester l'analyseur avec des donn√©es pr√©-charg√©es")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üèÉ Test Plat", use_container_width=True):
                df_final = generate_sample_data("plat")
                st.success("‚úÖ Donn√©es PLAT charg√©es (7 chevaux)")
        with col2:
            if st.button("üöó Test Attel√©", use_container_width=True):
                df_final = generate_sample_data("attele")
                st.success("‚úÖ Donn√©es ATTEL√â charg√©es (6 chevaux)")
        with col3:
            if st.button("‚≠ê Test Premium", use_container_width=True):
                df_final = generate_sample_data("premium")
                st.success("‚úÖ Donn√©es PREMIUM charg√©es (7 chevaux)")
        
        if df_final is not None:
            st.dataframe(df_final, use_container_width=True)
    
    # === ANALYSE PRINCIPALE ===
    if df_final is not None and len(df_final) > 0:
        st.markdown("---")
        st.header("üéØ Analyse et Pr√©dictions ML")
        
        df_prepared = prepare_data(df_final)
        if len(df_prepared) == 0:
            st.error("‚ùå Aucune donn√©e valide apr√®s pr√©paration")
            return
        
        # D√©tection du type de course
        if race_type == "AUTO":
            detected_type = auto_detect_race_type(df_prepared)
        else:
            detected_type = race_type
            st.info(f"üìã **Type s√©lectionn√©**: {CONFIGS[detected_type]['description']}")
        
        # === MACHINE LEARNING ===
        ml_model = AdvancedHorseRacingML()
        ml_results = None
        confidence_scores = None
        
        if use_ml:
            with st.spinner("ü§ñ Entra√Ænement des mod√®les ML avanc√©s..."):
                try:
                    # Pr√©paration des features avanc√©es
                    X_ml = ml_model.prepare_advanced_features(df_prepared, detected_type)
                    
                    # Affichage du nombre de features
                    st.info(f"üî¨ **{len(X_ml.columns)} features** cr√©√©es pour l'analyse ML")
                    
                    # Entra√Ænement et pr√©diction
                    ml_predictions, ml_results, confidence_scores = ml_model.train_and_predict(X_ml, detected_type)
                    
                    # Normalisation des pr√©dictions ML
                    if len(ml_predictions) > 0 and ml_predictions.max() != ml_predictions.min():
                        ml_predictions = (ml_predictions - ml_predictions.min()) / (ml_predictions.max() - ml_predictions.min())
                    
                    df_prepared['ml_score'] = ml_predictions
                    df_prepared['confidence'] = confidence_scores
                    
                    st.success("‚úÖ Mod√®les ML entra√Æn√©s avec succ√®s")
                    
                    # Affichage des m√©triques ML
                    if ml_results:
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            if 'stacking' in ml_results:
                                st.metric("üèÜ R¬≤ Stacking", f"{ml_results['stacking']['mean']:.3f}")
                        with col2:
                            if 'random_forest' in ml_results:
                                st.metric("üå≤ R¬≤ RF", f"{ml_results['random_forest']['mean']:.3f}")
                        with col3:
                            if 'gradient_boosting' in ml_results:
                                st.metric("üìà R¬≤ GB", f"{ml_results['gradient_boosting']['mean']:.3f}")
                        with col4:
                            avg_confidence = confidence_scores.mean() if confidence_scores is not None else 0
                            st.metric("üéØ Confiance Moy.", f"{avg_confidence:.1%}")
                    
                except Exception as e:
                    st.error(f"‚ö†Ô∏è Erreur ML: {e}")
                    use_ml = False
        
        # === SCORE TRADITIONNEL ===
        traditional_score = 1 / (df_prepared['odds_numeric'] + 0.1)
        if traditional_score.max() != traditional_score.min():
            traditional_score = (traditional_score - traditional_score.min()) / (traditional_score.max() - traditional_score.min())
        
        # === SCORE FINAL ===
        if use_ml and 'ml_score' in df_prepared.columns:
            df_prepared['score_final'] = (1 - ml_confidence) * traditional_score + ml_confidence * df_prepared['ml_score']
        else:
            df_prepared['score_final'] = traditional_score
            df_prepared['confidence'] = np.ones(len(df_prepared)) * 0.5
        
        # === CLASSEMENT ===
        df_ranked = df_prepared.sort_values('score_final', ascending=False).reset_index(drop=True)
        df_ranked['rang'] = range(1, len(df_ranked) + 1)
        
        # === AFFICHAGE DES R√âSULTATS ===
        st.markdown("---")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üèÜ Classement Final avec Confiance")
            
            # Pr√©paration du DataFrame d'affichage
            display_cols = ['rang', 'Nom', 'Cote', 'Num√©ro de corde']
            if 'Poids' in df_ranked.columns:
                display_cols.append('Poids')
            if 'score_final' in df_ranked.columns:
                display_cols.append('score_final')
            if 'confidence' in df_ranked.columns:
                display_cols.append('confidence')
            
            display_df = df_ranked[display_cols].copy()
            
            # Formatage
            if 'score_final' in display_df.columns:
                display_df['Score'] = display_df['score_final'].apply(lambda x: f"{x:.3f}")
                display_df = display_df.drop('score_final', axis=1)
            
            if 'confidence' in display_df.columns:
                display_df['Confiance'] = display_df['confidence'].apply(lambda x: f"{x:.1%}")
                display_df = display_df.drop('confidence', axis=1)
            
            # Coloration conditionnelle
            st.dataframe(
                display_df,
                use_container_width=True,
                height=400
            )
        
        with col2:
            st.subheader("üìä Statistiques de Course")
            
            # M√©triques globales
            favoris = len(df_ranked[df_ranked['odds_numeric'] < 5])
            outsiders = len(df_ranked[df_ranked['odds_numeric'] > 15])
            avg_confidence = df_ranked['confidence'].mean() if 'confidence' in df_ranked.columns else 0
            
            st.markdown(f'<div class="metric-card">‚≠ê Favoris (cote < 5)<br><strong>{favoris}</strong></div>', unsafe_allow_html=True)
            st.markdown(f'<div class="metric-card">üé≤ Outsiders (cote > 15)<br><strong>{outsiders}</strong></div>', unsafe_allow_html=True)
            st.markdown(f'<div class="metric-card">üéØ Confiance Moyenne<br><strong>{avg_confidence:.1%}</strong></div>', unsafe_allow_html=True)
            
            # Top 5 avec confiance
            st.subheader("ü•á Top 5 Pr√©dictions")
            for i in range(min(5, len(df_ranked))):
                horse = df_ranked.iloc[i]
                conf = horse.get('confidence', 0.5)
                
                # D√©termination de la classe de confiance
                if conf >= 0.7:
                    conf_class = "confidence-high"
                    conf_emoji = "üü¢"
                elif conf >= 0.4:
                    conf_class = "confidence-medium"
                    conf_emoji = "üü°"
                else:
                    conf_class = "confidence-low"
                    conf_emoji = "üî¥"
                
                st.markdown(f"""
                <div class="prediction-box">
                    <strong>{i+1}. {horse['Nom']}</strong><br>
                    üìä Cote: <strong>{horse['Cote']}</strong> | 
                    üéØ Score: <strong>{horse['score_final']:.3f}</strong><br>
                    {conf_emoji} Confiance: <span class="{conf_class}">{conf:.1%}</span>
                </div>
                """, unsafe_allow_html=True)
        
        # === VISUALISATIONS AVANC√âES ===
        st.markdown("---")
        st.subheader("üìä Visualisations et Analyses ML")
        
        fig = create_advanced_visualization(df_ranked, ml_model if use_ml else None)
        st.plotly_chart(fig, use_container_width=True)
        
        # === ANALYSE DES FEATURES ===
        if use_ml and ml_model.feature_importance:
            st.markdown("---")
            st.subheader("üî¨ Analyse de l'Importance des Features")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**üå≤ Random Forest - Top Features**")
                if 'random_forest' in ml_model.feature_importance:
                    importance_df = pd.DataFrame(
                        list(ml_model.feature_importance['random_forest'].items()),
                        columns=['Feature', 'Importance']
                    ).sort_values('Importance', ascending=False)
                    st.dataframe(importance_df, use_container_width=True, height=300)
            
            with col2:
                st.markdown("**üìà Gradient Boosting - Top Features**")
                if 'gradient_boosting' in ml_model.feature_importance:
                    importance_df = pd.DataFrame(
                        list(ml_model.feature_importance['gradient_boosting'].items()),
                        columns=['Feature', 'Importance']
                    ).sort_values('Importance', ascending=False)
                    st.dataframe(importance_df, use_container_width=True, height=300)
        
        # === RECOMMANDATIONS STRAT√âGIQUES ===
        st.markdown("---")
        st.subheader("üí° Recommandations Strat√©giques")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üéØ Chevaux √† Fort Potentiel**")
            high_value = df_ranked[
                (df_ranked['score_final'] > df_ranked['score_final'].quantile(0.6)) &
                (df_ranked['odds_numeric'] > 5) &
                (df_ranked['confidence'] > 0.5)
            ].head(3)
            
            if len(high_value) > 0:
                for idx, horse in high_value.iterrows():
                    st.success(f"‚úÖ **{horse['Nom']}** - Cote: {horse['Cote']} | Score: {horse['score_final']:.3f}")
            else:
                st.info("Aucun outsider √† fort potentiel d√©tect√©")
        
        with col2:
            st.markdown("**‚ö†Ô∏è Alertes et Observations**")
            
            # Alerte sur les favoris sous-performants
            weak_favorites = df_ranked[
                (df_ranked['odds_numeric'] < 5) &
                (df_ranked['score_final'] < df_ranked['score_final'].median())
            ]
            
            if len(weak_favorites) > 0:
                st.warning(f"‚ö†Ô∏è {len(weak_favorites)} favori(s) avec score faible")
            
            # Surprise potentielle
            surprise = df_ranked[
                (df_ranked['odds_numeric'] > 10) &
                (df_ranked['rang'] <= 3)
            ]
            
            if len(surprise) > 0:
                st.info(f"üé≤ {len(surprise)} outsider(s) dans le Top 3!")
            else:
                st.info("‚úÖ Classement coh√©rent avec les cotes")
        
        # === EXPORT DES R√âSULTATS ===
        st.markdown("---")
        st.subheader("üíæ Export des R√©sultats")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            csv_data = df_ranked.to_csv(index=False)
            st.download_button(
                "üìÑ T√©l√©charger CSV",
                csv_data,
                f"pronostic_ml_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv",
                use_container_width=True
            )
        
        with col2:
            json_data = df_ranked.to_json(orient='records', indent=2)
            st.download_button(
                "üìã T√©l√©charger JSON",
                json_data,
                f"pronostic_ml_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                "application/json",
                use_container_width=True
            )
        
        with col3:
            # Export du rapport complet
            report = f"""
RAPPORT D'ANALYSE HIPPIQUE ML
{'='*50}
Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Type de course: {detected_type}
Nombre de chevaux: {len(df_ranked)}

TOP 5 PR√âDICTIONS:
{'-'*50}
"""
            for i in range(min(5, len(df_ranked))):
                horse = df_ranked.iloc[i]
                report += f"{i+1}. {horse['Nom']} - Cote: {horse['Cote']} - Score: {horse['score_final']:.3f}\n"
            
            if ml_results:
                report += f"\n{'='*50}\nM√âTRIQUES ML:\n{'-'*50}\n"
                for model, scores in ml_results.items():
                    report += f"{model}: R¬≤ = {scores['mean']:.3f} (+/- {scores['std']:.3f})\n"
            
            st.download_button(
                "üìä T√©l√©charger Rapport",
                report,
                f"rapport_ml_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                "text/plain",
                use_container_width=True
            )

if __name__ == "__main__":
    main()
